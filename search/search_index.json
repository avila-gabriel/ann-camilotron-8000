{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ann-camilotron-8000","text":"<p>Uma rede neural artificial simples, vetorizada com JAX e implementada com backpropaga\u00e7\u00e3o manual. Desenvolvido como um projeto acad\u00eamico de aprendizado de m\u00e1quina.</p>"},{"location":"#destaques","title":"Destaques","text":"<ul> <li>Totalmente vetorizada com <code>jax.numpy</code></li> <li>Inicializa\u00e7\u00e3o com He/Xavier</li> <li>Fun\u00e7\u00f5es de ativa\u00e7\u00e3o: <code>relu</code>, <code>sigmoid</code>, <code>softmax</code>, <code>linear</code></li> <li>Tr\u00eas fun\u00e7\u00f5es de erro: bin\u00e1ria, categorial, MSE</li> <li>Treinamento com mini-batch via <code>treinar_rede(...)</code></li> <li>Backpropaga\u00e7\u00e3o feita na unha (sem <code>jax.grad</code>)</li> <li>Documenta\u00e7\u00e3o com mkdocstrings</li> </ul>"},{"location":"#exemplos-de-uso","title":"Exemplos de Uso","text":"<p>Explore os notebooks para ver aplica\u00e7\u00f5es pr\u00e1ticas da rede em problemas como classifica\u00e7\u00e3o bin\u00e1ria e regress\u00e3o linear.</p> <ul> <li>Classifica\u00e7\u00e3o banc\u00e1ria</li> <li>Regress\u00e3o imobili\u00e1ria</li> </ul>"},{"location":"#sobre-o-projeto","title":"Sobre o projeto","text":"<p>Este projeto foi desenvolvido como exerc\u00edcio pr\u00e1tico para compreender os fundamentos de redes neurais e backpropaga\u00e7\u00e3o vetorizada. O c\u00f3digo \u00e9 curto, direto, e foi constru\u00eddo com foco em clareza, n\u00e3o em performance ou produ\u00e7\u00e3o.</p> <ul> <li>Sem bibliotecas de alto n\u00edvel como Keras/PyTorch</li> <li>Sem uso de <code>jax.grad</code> ou autodiff</li> <li>Ideal para fins did\u00e1ticos e experimenta\u00e7\u00e3o</li> </ul>"},{"location":"#codigo-fonte","title":"C\u00f3digo Fonte","text":"<p>O reposit\u00f3rio est\u00e1 dispon\u00edvel no GitHub em: github.com/avila-gabriel/ann-camilotron-8000</p>"},{"location":"#referencia-da-api","title":"Refer\u00eancia da API","text":"<p>Veja a documenta\u00e7\u00e3o completa das fun\u00e7\u00f5es em API Reference.</p>"},{"location":"#ambiente-de-desenvolvimento","title":"Ambiente de Desenvolvimento","text":"<p>Consulte Ambiente de Desenvolvimento para saber como instalar depend\u00eancias, configurar o ambiente e trabalhar localmente com o projeto.</p>"},{"location":"#autores","title":"Autores","text":"<ul> <li>Gabriel Avila</li> <li>Carlos Botelho </li> <li>Juan Lopes </li> </ul>"},{"location":"#licenca","title":"Licen\u00e7a","text":"<p>MIT \u00a9 2025 \u2014 Projeto acad\u00eamico para fins educacionais.</p>"},{"location":"dev/","title":"Contributing","text":""},{"location":"dev/#prerequisites","title":"Prerequisites","text":"<ul> <li>Install <code>uv</code></li> <li>In VS Code, install the following extensions:</li> <li><code>charliermarsh.ruff</code></li> <li><code>meta.pyrefly</code></li> </ul>"},{"location":"dev/#setup","title":"Setup","text":"<p>Install all project dependencies, including dev tools:</p> <pre><code>uv sync\n</code></pre> <p>then activate the venv</p>"},{"location":"dev/#adding-a-library","title":"Adding a Library","text":"<p>To add a new library:</p> <pre><code>uv add &lt;LIB&gt;\n</code></pre>"},{"location":"reference/","title":"API Reference","text":"<p>Rede Neural Artificial</p> <ul> <li>Vetorizada em jax.numpy</li> <li>Backpropaga\u00e7\u00e3o manual (sem jax.grad)</li> </ul>"},{"location":"reference/#ann.atualizar_parametros","title":"atualizar_parametros","text":"<pre><code>atualizar_parametros(\n    parametros_rede, gradientes_parametros, taxa_aprendizado\n)\n</code></pre> <p>Returns um novo dicion\u00e1rio de pesos e vieses da rede, ap\u00f3s atualiza\u00e7\u00e3o via gradiente descendente.</p> <p>Parameters:</p> Name Type Description Default <code>parametros_rede</code> <code>dict[str, ndarray]</code> <p>Parameters atuais da rede.</p> required <code>gradientes_parametros</code> <code>dict[str, ndarray]</code> <p>Gradientes de cada par\u00e2metro.</p> required <code>taxa_aprendizado</code> <code>float</code> <p>Passo do gradiente descendente.</p> required <p>Returns:</p> Type Description <code>dict[str, ndarray]</code> <p>Novo dicion\u00e1rio de Parameters ap\u00f3s atualiza\u00e7\u00e3o.</p> Source code in <code>src\\ann.py</code> <pre><code>def atualizar_parametros(\n    parametros_rede: dict[str, jnp.ndarray],\n    gradientes_parametros: dict[str, jnp.ndarray],\n    taxa_aprendizado: float,\n) -&gt; dict[str, jnp.ndarray]:\n    \"\"\"\n    Returns um novo dicion\u00e1rio de pesos e vieses da rede, ap\u00f3s atualiza\u00e7\u00e3o via gradiente\n    descendente.\n\n    Parameters\n    ----------\n    parametros_rede : dict[str, jnp.ndarray]\n        Parameters atuais da rede.\n    gradientes_parametros : dict[str, jnp.ndarray]\n        Gradientes de cada par\u00e2metro.\n    taxa_aprendizado : float\n        Passo do gradiente descendente.\n\n    Returns\n    -------\n    dict[str, jnp.ndarray]\n        Novo dicion\u00e1rio de Parameters ap\u00f3s atualiza\u00e7\u00e3o.\n    \"\"\"\n    novo_parametros: dict[str, Array] = {}\n    for indice_camada in range(1, len(parametros_rede) // 2 + 1):\n        novo_parametros[f\"W{indice_camada}\"] = (\n            parametros_rede[f\"W{indice_camada}\"]\n            - taxa_aprendizado * gradientes_parametros[f\"dW{indice_camada}\"]\n        )\n        novo_parametros[f\"b{indice_camada}\"] = (\n            parametros_rede[f\"b{indice_camada}\"]\n            - taxa_aprendizado * gradientes_parametros[f\"db{indice_camada}\"]\n        )\n    return novo_parametros\n</code></pre>"},{"location":"reference/#ann.codificar_one_hot","title":"codificar_one_hot","text":"<pre><code>codificar_one_hot(rotulos_inteiros, numero_classes)\n</code></pre> <p>Converte um vetor de r\u00f3tulos inteiros para codifica\u00e7\u00e3o one-hot.</p> <p>Parameters:</p> Name Type Description Default <code>rotulos_inteiros</code> <code>ndarray</code> <p>Array 1D contendo os r\u00f3tulos (valores inteiros).</p> required <code>numero_classes</code> <code>int</code> <p>N\u00famero total de classes poss\u00edveis.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Matriz one-hot de forma (numero_classes, n_amostras).</p> Source code in <code>src\\ann.py</code> <pre><code>def codificar_one_hot(\n    rotulos_inteiros: jnp.ndarray, numero_classes: int\n) -&gt; jnp.ndarray:\n    \"\"\"\n    Converte um vetor de r\u00f3tulos inteiros para codifica\u00e7\u00e3o one-hot.\n\n    Parameters\n    ----------\n    rotulos_inteiros : jnp.ndarray\n        Array 1D contendo os r\u00f3tulos (valores inteiros).\n    numero_classes : int\n        N\u00famero total de classes poss\u00edveis.\n\n    Returns\n    -------\n    jnp.ndarray\n        Matriz one-hot de forma (numero_classes, n_amostras).\n    \"\"\"\n    matriz_one_hot = jnp.zeros((numero_classes, rotulos_inteiros.size))\n    matriz_one_hot = matriz_one_hot.at[\n        rotulos_inteiros, jnp.arange(rotulos_inteiros.size)\n    ].set(1.0)\n    return matriz_one_hot\n</code></pre>"},{"location":"reference/#ann.definir_semente","title":"definir_semente","text":"<pre><code>definir_semente(semente=42)\n</code></pre> <p>Gera chave de aleatoriedade JAX a partir de uma semente para reprodutibilidade.</p> <p>Parameters:</p> Name Type Description Default <code>semente</code> <code>(int, opcional)</code> <p>Valor inteiro utilizado para inicializa\u00e7\u00e3o do gerador de n\u00fameros aleat\u00f3rios.</p> <code>42</code> <p>Returns:</p> Type Description <code>PRNGKey</code> <p>Chave para gera\u00e7\u00e3o de n\u00fameros aleat\u00f3rios no JAX.</p> Source code in <code>src\\ann.py</code> <pre><code>def definir_semente(semente: int = 42) -&gt; Array:\n    \"\"\"\n    Gera chave de aleatoriedade JAX a partir de uma semente para reprodutibilidade.\n\n    Parameters\n    ----------\n    semente : int, opcional\n        Valor inteiro utilizado para inicializa\u00e7\u00e3o do gerador de n\u00fameros aleat\u00f3rios.\n\n    Returns\n    -------\n    jax.random.PRNGKey\n        Chave para gera\u00e7\u00e3o de n\u00fameros aleat\u00f3rios no JAX.\n    \"\"\"\n    return jax.random.key(semente)\n</code></pre>"},{"location":"reference/#ann.erro_binario_cruzado","title":"erro_binario_cruzado","text":"<pre><code>erro_binario_cruzado(\n    probabilidades_preditas, rotulos_reais\n)\n</code></pre> <p>Calcula o erro bin\u00e1rio cruzado para classifica\u00e7\u00e3o bin\u00e1ria.</p> <p>Parameters:</p> Name Type Description Default <code>probabilidades_preditas</code> <code>ndarray</code> <p>Sa\u00edda da rede ap\u00f3s sigm\u00f3ide (dim: 1, n_amostras).</p> required <code>rotulos_reais</code> <code>ndarray</code> <p>R\u00f3tulos reais (dim: 1, n_amostras).</p> required <p>Returns:</p> Type Description <code>float</code> <p>Valor escalar do erro m\u00e9dio.</p> Source code in <code>src\\ann.py</code> <pre><code>def erro_binario_cruzado(\n    probabilidades_preditas: jnp.ndarray, rotulos_reais: jnp.ndarray\n) -&gt; float:\n    \"\"\"\n    Calcula o erro bin\u00e1rio cruzado para classifica\u00e7\u00e3o bin\u00e1ria.\n\n    Parameters\n    ----------\n    probabilidades_preditas : jnp.ndarray\n        Sa\u00edda da rede ap\u00f3s sigm\u00f3ide (dim: 1, n_amostras).\n    rotulos_reais : jnp.ndarray\n        R\u00f3tulos reais (dim: 1, n_amostras).\n\n    Returns\n    -------\n    float\n        Valor escalar do erro m\u00e9dio.\n    \"\"\"\n    eps = 1e-12\n    return float(\n        jnp.mean(\n            -(\n                rotulos_reais * jnp.log(probabilidades_preditas + eps)\n                + (1 - rotulos_reais) * jnp.log(1 - probabilidades_preditas + eps)\n            )\n        )\n    )\n</code></pre>"},{"location":"reference/#ann.erro_categorial_cruzado","title":"erro_categorial_cruzado","text":"<pre><code>erro_categorial_cruzado(\n    probabilidades_preditas, rotulos_one_hot\n)\n</code></pre> <p>Calcula o erro categorial cruzado para classifica\u00e7\u00e3o multiclasse.</p> <p>Parameters:</p> Name Type Description Default <code>probabilidades_preditas</code> <code>ndarray</code> <p>Sa\u00eddas da rede ap\u00f3s softmax (classes, n_amostras).</p> required <code>rotulos_one_hot</code> <code>ndarray</code> <p>R\u00f3tulos reais (codifica\u00e7\u00e3o one-hot).</p> required <p>Returns:</p> Type Description <code>float</code> <p>Valor escalar do erro m\u00e9dio.</p> Source code in <code>src\\ann.py</code> <pre><code>def erro_categorial_cruzado(\n    probabilidades_preditas: jnp.ndarray, rotulos_one_hot: jnp.ndarray\n) -&gt; float:\n    \"\"\"\n    Calcula o erro categorial cruzado para classifica\u00e7\u00e3o multiclasse.\n\n    Parameters\n    ----------\n    probabilidades_preditas : jnp.ndarray\n        Sa\u00eddas da rede ap\u00f3s softmax (classes, n_amostras).\n    rotulos_one_hot : jnp.ndarray\n        R\u00f3tulos reais (codifica\u00e7\u00e3o one-hot).\n\n    Returns\n    -------\n    float\n        Valor escalar do erro m\u00e9dio.\n    \"\"\"\n    eps = 1e-12\n    return float(\n        -jnp.sum(rotulos_one_hot * jnp.log(probabilidades_preditas + eps))\n        / rotulos_one_hot.shape[1]\n    )\n</code></pre>"},{"location":"reference/#ann.erro_mse","title":"erro_mse","text":"<pre><code>erro_mse(predicoes, rotulos_reais)\n</code></pre> <p>Calcula o erro quadr\u00e1tico m\u00e9dio (Mean Squared Error) para tarefas de regress\u00e3o.</p> <p>Parameters:</p> Name Type Description Default <code>predicoes</code> <code>ndarray</code> <p>Valores preditos pelo modelo (dim: 1, n_amostras ou similar).</p> required <code>rotulos_reais</code> <code>ndarray</code> <p>Valores reais esperados (dim: 1, n_amostras ou similar).</p> required <p>Returns:</p> Type Description <code>float</code> <p>Valor escalar do erro quadr\u00e1tico m\u00e9dio.</p> Source code in <code>src\\ann.py</code> <pre><code>def erro_mse(predicoes: jnp.ndarray, rotulos_reais: jnp.ndarray) -&gt; float:\n    \"\"\"\n    Calcula o erro quadr\u00e1tico m\u00e9dio (Mean Squared Error) para tarefas de regress\u00e3o.\n\n    Parameters\n    ----------\n    predicoes : jnp.ndarray\n        Valores preditos pelo modelo (dim: 1, n_amostras ou similar).\n    rotulos_reais : jnp.ndarray\n        Valores reais esperados (dim: 1, n_amostras ou similar).\n\n    Returns\n    -------\n    float\n        Valor escalar do erro quadr\u00e1tico m\u00e9dio.\n    \"\"\"\n    return float(jnp.mean((predicoes - rotulos_reais) ** 2))\n</code></pre>"},{"location":"reference/#ann.inicializar_parametros_rede","title":"inicializar_parametros_rede","text":"<pre><code>inicializar_parametros_rede(\n    dimensoes_camadas,\n    *,\n    chave_aleatoria,\n    nome_ativacao_oculta=\"relu\",\n)\n</code></pre> <p>Inicializa pesos e vieses de todas as camadas da rede neural.</p> <p>A estrat\u00e9gia de inicializa\u00e7\u00e3o varia conforme a ativa\u00e7\u00e3o oculta: - Para ReLU: inicializa\u00e7\u00e3o de He (\u221a(2/fan_in)). - Para outras ativa\u00e7\u00f5es (sigmoid, tanh): inicializa\u00e7\u00e3o de Xavier (\u221a(1/fan_in)). Al\u00e9m disso, o vi\u00e9s da primeira camada \u00e9 inicializado com um valor pequeno e positivo (0.01) quando se usa ReLU, para evitar \u201cneur\u00f4nios mortos\u201d no in\u00edcio.</p> <p>Parameters:</p> Name Type Description Default <code>dimensoes_camadas</code> <code>Sequence[int]</code> <p>Sequ\u00eancia contendo o n\u00famero de unidades de cada camada, incluindo camada de entrada e camada de sa\u00edda.</p> required <code>chave_aleatoria</code> <code>Array</code> <p>Chave PRNG do JAX para gera\u00e7\u00e3o de n\u00fameros aleat\u00f3rios.</p> required <code>nome_ativacao_oculta</code> <code>('relu', 'sigmoid', 'tanh')</code> <p>Especifica a fun\u00e7\u00e3o de ativa\u00e7\u00e3o das camadas ocultas; define o m\u00e9todo de inicializa\u00e7\u00e3o de pesos. Padr\u00e3o \u00e9 'relu'.</p> <code>'relu'</code> <p>Returns:</p> Name Type Description <code>parametros_rede</code> <code>dict[str, ndarray]</code> <p>Dicion\u00e1rio com Parameters aprendidos da rede: - W1, b1, W2, b2, \u2026 at\u00e9 WL, bL. Cada W{i} tem forma (dimensoes_camadas[i], dimensoes_camadas[i-1]), e cada b{i} tem forma (dimensoes_camadas[i], 1).</p> <code>chave_aleatoria</code> <code>Array</code> <p>Nova chave PRNG resultante ap\u00f3s as divis\u00f5es, para uso em chamadas subsequentes.</p> Source code in <code>src\\ann.py</code> <pre><code>def inicializar_parametros_rede(\n    dimensoes_camadas: Sequence[int],\n    *,\n    chave_aleatoria: Array,\n    nome_ativacao_oculta: str = \"relu\",\n) -&gt; tuple[dict[str, jnp.ndarray], Array]:\n    \"\"\"\n    Inicializa pesos e vieses de todas as camadas da rede neural.\n\n    A estrat\u00e9gia de inicializa\u00e7\u00e3o varia conforme a ativa\u00e7\u00e3o oculta:\n    - Para ReLU: inicializa\u00e7\u00e3o de He (\u221a(2/fan_in)).\n    - Para outras ativa\u00e7\u00f5es (sigmoid, tanh): inicializa\u00e7\u00e3o de Xavier (\u221a(1/fan_in)).\n    Al\u00e9m disso, o vi\u00e9s da primeira camada \u00e9 inicializado com um valor\n    pequeno e positivo (0.01) quando se usa ReLU, para evitar \u201cneur\u00f4nios\n    mortos\u201d no in\u00edcio.\n\n    Parameters\n    ----------\n    dimensoes_camadas : Sequence[int]\n        Sequ\u00eancia contendo o n\u00famero de unidades de cada camada,\n        incluindo camada de entrada e camada de sa\u00edda.\n    chave_aleatoria : Array\n        Chave PRNG do JAX para gera\u00e7\u00e3o de n\u00fameros aleat\u00f3rios.\n    nome_ativacao_oculta : {'relu', 'sigmoid', 'tanh'}, opcional\n        Especifica a fun\u00e7\u00e3o de ativa\u00e7\u00e3o das camadas ocultas; define\n        o m\u00e9todo de inicializa\u00e7\u00e3o de pesos. Padr\u00e3o \u00e9 'relu'.\n\n    Returns\n    -------\n    parametros_rede : dict[str, jnp.ndarray]\n        Dicion\u00e1rio com Parameters aprendidos da rede:\n        - W1, b1, W2, b2, \u2026 at\u00e9 WL, bL.\n        Cada W{i} tem forma (dimensoes_camadas[i], dimensoes_camadas[i-1]),\n        e cada b{i} tem forma (dimensoes_camadas[i], 1).\n    chave_aleatoria : Array\n        Nova chave PRNG resultante ap\u00f3s as divis\u00f5es, para uso em\n        chamadas subsequentes.\n    \"\"\"\n    parametros: dict[str, jnp.ndarray] = {}\n    for i in range(1, len(dimensoes_camadas)):\n        chave_aleatoria, subkey = jax.random.split(chave_aleatoria)\n        fan_in, fan_out = dimensoes_camadas[i - 1], dimensoes_camadas[i]\n        if nome_ativacao_oculta == \"relu\":\n            limite = jnp.sqrt(2.0 / fan_in)  # He\n        else:\n            limite = jnp.sqrt(1.0 / fan_in)  # Xavier\n        parametros[f\"W{i}\"] = jax.random.normal(subkey, (fan_out, fan_in)) * limite\n        parametros[f\"b{i}\"] = (\n            0.01 if (i == 1 and nome_ativacao_oculta == \"relu\") else 0.0\n        ) * jnp.ones((fan_out, 1))\n    return parametros, chave_aleatoria\n</code></pre>"},{"location":"reference/#ann.linear","title":"linear","text":"<pre><code>linear(entrada_linear)\n</code></pre> <p>Aplica a fun\u00e7\u00e3o de ativa\u00e7\u00e3o linear (identidade).</p> <p>Esta fun\u00e7\u00e3o \u00e9 utilizada principalmente em problemas de regress\u00e3o, onde n\u00e3o se deseja transformar a sa\u00edda da rede, apenas propag\u00e1-la diretamente como predi\u00e7\u00e3o cont\u00ednua.</p> <p>Parameters:</p> Name Type Description Default <code>entrada_linear</code> <code>ndarray</code> <p>Vetor ou matriz com os valores de entrada da camada (pr\u00e9-ativa\u00e7\u00e3o).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Mesmo array de entrada, sem altera\u00e7\u00f5es.</p> Source code in <code>src\\ann.py</code> <pre><code>def linear(entrada_linear: jnp.ndarray) -&gt; jnp.ndarray:\n    \"\"\"\n    Aplica a fun\u00e7\u00e3o de ativa\u00e7\u00e3o linear (identidade).\n\n    Esta fun\u00e7\u00e3o \u00e9 utilizada principalmente em problemas de regress\u00e3o,\n    onde n\u00e3o se deseja transformar a sa\u00edda da rede, apenas propag\u00e1-la\n    diretamente como predi\u00e7\u00e3o cont\u00ednua.\n\n    Parameters\n    ----------\n    entrada_linear : jnp.ndarray\n        Vetor ou matriz com os valores de entrada da camada (pr\u00e9-ativa\u00e7\u00e3o).\n\n    Returns\n    -------\n    jnp.ndarray\n        Mesmo array de entrada, sem altera\u00e7\u00f5es.\n    \"\"\"\n    return entrada_linear\n</code></pre>"},{"location":"reference/#ann.linear_derivada","title":"linear_derivada","text":"<pre><code>linear_derivada(gradiente_saida, entrada_linear)\n</code></pre> <p>Calcula o gradiente da fun\u00e7\u00e3o de ativa\u00e7\u00e3o linear (identidade).</p> <p>Como a fun\u00e7\u00e3o linear \u00e9 definida por f(x) = x, sua derivada \u00e9 1. Logo, o gradiente em rela\u00e7\u00e3o \u00e0 entrada linear \u00e9 simplesmente igual ao gradiente da camada posterior.</p> <p>Parameters:</p> Name Type Description Default <code>gradiente_saida</code> <code>ndarray</code> <p>Gradiente do erro em rela\u00e7\u00e3o \u00e0 sa\u00edda da fun\u00e7\u00e3o de ativa\u00e7\u00e3o.</p> required <code>entrada_linear</code> <code>ndarray</code> <p>Valor da entrada linear (antes da ativa\u00e7\u00e3o); n\u00e3o \u00e9 utilizado no c\u00e1lculo, mas inclu\u00eddo por compatibilidade.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Gradiente do erro em rela\u00e7\u00e3o \u00e0 entrada da fun\u00e7\u00e3o de ativa\u00e7\u00e3o.</p> Source code in <code>src\\ann.py</code> <pre><code>def linear_derivada(\n    gradiente_saida: jnp.ndarray, entrada_linear: jnp.ndarray\n) -&gt; jnp.ndarray:\n    \"\"\"\n    Calcula o gradiente da fun\u00e7\u00e3o de ativa\u00e7\u00e3o linear (identidade).\n\n    Como a fun\u00e7\u00e3o linear \u00e9 definida por f(x) = x, sua derivada \u00e9 1.\n    Logo, o gradiente em rela\u00e7\u00e3o \u00e0 entrada linear \u00e9 simplesmente igual\n    ao gradiente da camada posterior.\n\n    Parameters\n    ----------\n    gradiente_saida : jnp.ndarray\n        Gradiente do erro em rela\u00e7\u00e3o \u00e0 sa\u00edda da fun\u00e7\u00e3o de ativa\u00e7\u00e3o.\n    entrada_linear : jnp.ndarray\n        Valor da entrada linear (antes da ativa\u00e7\u00e3o); n\u00e3o \u00e9 utilizado\n        no c\u00e1lculo, mas inclu\u00eddo por compatibilidade.\n\n    Returns\n    -------\n    jnp.ndarray\n        Gradiente do erro em rela\u00e7\u00e3o \u00e0 entrada da fun\u00e7\u00e3o de ativa\u00e7\u00e3o.\n    \"\"\"\n    return gradiente_saida\n</code></pre>"},{"location":"reference/#ann.prever","title":"prever","text":"<pre><code>prever(\n    matriz_entrada,\n    parametros_rede,\n    *,\n    nome_ativacao_oculta=\"relu\",\n    nome_ativacao_saida=\"sigmoid\",\n)\n</code></pre> <p>Realiza predi\u00e7\u00e3o dos r\u00f3tulos utilizando os Parameters aprendidos.</p> <p>Parameters:</p> Name Type Description Default <code>matriz_entrada</code> <code>ndarray</code> <p>Dados de entrada (n_features, n_amostras).</p> required <code>parametros_rede</code> <code>dict[str, ndarray]</code> <p>Parameters aprendidos da rede.</p> required <code>nome_ativacao_oculta</code> <code>('relu', 'sigmoid', 'tanh')</code> <p>Fun\u00e7\u00e3o de ativa\u00e7\u00e3o das camadas ocultas.</p> <code>'relu'</code> <code>nome_ativacao_saida</code> <code>('sigmoid', 'softmax', 'linear')</code> <p>Fun\u00e7\u00e3o de ativa\u00e7\u00e3o da camada de sa\u00edda.</p> <code>'sigmoid'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>R\u00f3tulos preditos (0/1, \u00edndice da classe, ou valores cont\u00ednuos).</p> Source code in <code>src\\ann.py</code> <pre><code>def prever(\n    matriz_entrada: jnp.ndarray,\n    parametros_rede: dict[str, jnp.ndarray],\n    *,\n    nome_ativacao_oculta: str = \"relu\",\n    nome_ativacao_saida: str = \"sigmoid\",\n) -&gt; jnp.ndarray:\n    \"\"\"\n    Realiza predi\u00e7\u00e3o dos r\u00f3tulos utilizando os Parameters aprendidos.\n\n    Parameters\n    ----------\n    matriz_entrada : jnp.ndarray\n        Dados de entrada (n_features, n_amostras).\n    parametros_rede : dict[str, jnp.ndarray]\n        Parameters aprendidos da rede.\n    nome_ativacao_oculta : {'relu', 'sigmoid', 'tanh'}, opcional\n        Fun\u00e7\u00e3o de ativa\u00e7\u00e3o das camadas ocultas.\n    nome_ativacao_saida : {'sigmoid', 'softmax', 'linear'}, opcional\n        Fun\u00e7\u00e3o de ativa\u00e7\u00e3o da camada de sa\u00edda.\n\n    Returns\n    -------\n    jnp.ndarray\n        R\u00f3tulos preditos (0/1, \u00edndice da classe, ou valores cont\u00ednuos).\n    \"\"\"\n    probabilidades_preditas, _ = propagacao_ante(\n        matriz_entrada,\n        parametros_rede,\n        nome_ativacao_oculta=nome_ativacao_oculta,\n        nome_ativacao_saida=nome_ativacao_saida,\n    )\n    if nome_ativacao_saida == \"sigmoid\":\n        return (probabilidades_preditas &gt;= 0.5).astype(int)\n    elif nome_ativacao_saida == \"linear\":\n        return probabilidades_preditas\n    else:\n        return jnp.argmax(probabilidades_preditas, axis=0)\n</code></pre>"},{"location":"reference/#ann.propagacao_ante","title":"propagacao_ante","text":"<pre><code>propagacao_ante(\n    matriz_entrada,\n    parametros_rede,\n    *,\n    nome_ativacao_oculta=\"relu\",\n    nome_ativacao_saida=\"sigmoid\",\n)\n</code></pre> <p>Executa a passagem para frente na rede, armazenando intermedi\u00e1rios.</p> <p>Parameters:</p> Name Type Description Default <code>matriz_entrada</code> <code>ndarray</code> <p>Dados de entrada (n_features, n_amostras).</p> required <code>parametros_rede</code> <code>dict[str, ndarray]</code> <p>Dicion\u00e1rio com todos os pesos/vi\u00e9ses da rede.</p> required <code>nome_ativacao_oculta</code> <code>('relu', 'sigmoid', 'tanh')</code> <p>Fun\u00e7\u00e3o de ativa\u00e7\u00e3o das camadas ocultas.</p> <code>'relu'</code> <code>nome_ativacao_saida</code> <code>('sigmoid', 'softmax', 'linear')</code> <p>Fun\u00e7\u00e3o de ativa\u00e7\u00e3o da camada de sa\u00edda.</p> <code>'sigmoid'</code> <p>Returns:</p> Name Type Description <code>matriz_predicoes</code> <code>ndarray</code> <p>Sa\u00edda final da rede (probabilidades, logits ou valores cont\u00ednuos).</p> <code>cache_propagacao</code> <code>dict[str, ndarray]</code> <p>Intermedi\u00e1rios (A0, Z1, A1, ...).</p> Source code in <code>src\\ann.py</code> <pre><code>def propagacao_ante(\n    matriz_entrada: jnp.ndarray,\n    parametros_rede: dict[str, jnp.ndarray],\n    *,\n    nome_ativacao_oculta: str = \"relu\",\n    nome_ativacao_saida: str = \"sigmoid\",\n) -&gt; tuple[jnp.ndarray, dict[str, jnp.ndarray]]:\n    \"\"\"\n    Executa a passagem para frente na rede, armazenando intermedi\u00e1rios.\n\n    Parameters\n    ----------\n    matriz_entrada : jnp.ndarray\n        Dados de entrada (n_features, n_amostras).\n    parametros_rede : dict[str, jnp.ndarray]\n        Dicion\u00e1rio com todos os pesos/vi\u00e9ses da rede.\n    nome_ativacao_oculta : {'relu', 'sigmoid', 'tanh'}\n        Fun\u00e7\u00e3o de ativa\u00e7\u00e3o das camadas ocultas.\n    nome_ativacao_saida : {'sigmoid', 'softmax', 'linear'}\n        Fun\u00e7\u00e3o de ativa\u00e7\u00e3o da camada de sa\u00edda.\n\n    Returns\n    -------\n    matriz_predicoes : jnp.ndarray\n        Sa\u00edda final da rede (probabilidades, logits ou valores cont\u00ednuos).\n    cache_propagacao : dict[str, jnp.ndarray]\n        Intermedi\u00e1rios (A0, Z1, A1, ...).\n    \"\"\"\n    cache_propagacao: dict[str, jnp.ndarray] = {\"A0\": matriz_entrada}\n    ativacoes = matriz_entrada\n    numero_camadas = len(parametros_rede) // 2\n\n    mapa_ativacao: dict[str, Callable[[jnp.ndarray], jnp.ndarray]] = {\n        \"relu\": relu,\n        \"sigmoid\": sigmoid,\n        \"softmax\": softmax,\n        \"linear\": linear,\n    }\n\n    for indice_camada in range(1, numero_camadas):\n        entrada_linear = (\n            parametros_rede[f\"W{indice_camada}\"] @ ativacoes\n            + parametros_rede[f\"b{indice_camada}\"]\n        )\n        ativacoes = mapa_ativacao[nome_ativacao_oculta](entrada_linear)\n        cache_propagacao[f\"Z{indice_camada}\"] = entrada_linear\n        cache_propagacao[f\"A{indice_camada}\"] = ativacoes\n\n    entrada_linear_saida = (\n        parametros_rede[f\"W{numero_camadas}\"] @ ativacoes\n        + parametros_rede[f\"b{numero_camadas}\"]\n    )\n    predicoes = mapa_ativacao[nome_ativacao_saida](entrada_linear_saida)\n    cache_propagacao[f\"Z{numero_camadas}\"] = entrada_linear_saida\n    cache_propagacao[f\"A{numero_camadas}\"] = predicoes\n    return predicoes, cache_propagacao\n</code></pre>"},{"location":"reference/#ann.propagacao_retro","title":"propagacao_retro","text":"<pre><code>propagacao_retro(\n    parametros_rede,\n    cache_propagacao,\n    rotulos_reais,\n    *,\n    nome_ativacao_oculta=\"relu\",\n)\n</code></pre> <p>Calcula o gradiente do erro em rela\u00e7\u00e3o a cada par\u00e2metro, unificando dZ = A - Y para sa\u00edda (v\u00e1lido tanto p/ sigmoid quanto softmax).</p> <p>Parameters:</p> Name Type Description Default <code>parametros_rede</code> <code>dict[str, ndarray]</code> <p>Pesos e vi\u00e9ses da rede.</p> required <code>cache_propagacao</code> <code>dict[str, ndarray]</code> <p>Intermedi\u00e1rios da forward pass (Z1, A1, ..., ZL, AL).</p> required <code>rotulos_reais</code> <code>ndarray</code> <p>R\u00f3tulos verdadeiros (bin\u00e1rio 1 x m ou one-hot n_classes x m).</p> required <code>nome_ativacao_oculta</code> <code>('relu', 'sigmoid')</code> <p>Ativa\u00e7\u00e3o usada nas camadas ocultas.</p> <code>'relu'</code> <p>Returns:</p> Type Description <code>dict[str, ndarray]</code> <p>Gradientes dW1\u2026dWL, db1\u2026dbL normalizados por m.</p> Source code in <code>src\\ann.py</code> <pre><code>def propagacao_retro(\n    parametros_rede: dict[str, jnp.ndarray],\n    cache_propagacao: dict[str, jnp.ndarray],\n    rotulos_reais: jnp.ndarray,\n    *,\n    nome_ativacao_oculta: str = \"relu\",\n) -&gt; dict[str, jnp.ndarray]:\n    \"\"\"\n    Calcula o gradiente do erro em rela\u00e7\u00e3o a cada par\u00e2metro, unificando\n    dZ = A - Y para sa\u00edda (v\u00e1lido tanto p/ sigmoid quanto softmax).\n\n    Parameters\n    ----------\n    parametros_rede : dict[str, jnp.ndarray]\n        Pesos e vi\u00e9ses da rede.\n    cache_propagacao : dict[str, jnp.ndarray]\n        Intermedi\u00e1rios da forward pass (Z1, A1, ..., ZL, AL).\n    rotulos_reais : jnp.ndarray\n        R\u00f3tulos verdadeiros (bin\u00e1rio 1 x m ou one-hot n_classes x m).\n    nome_ativacao_oculta : {'relu', 'sigmoid'}, opcional\n        Ativa\u00e7\u00e3o usada nas camadas ocultas.\n\n    Returns\n    -------\n    dict[str, jnp.ndarray]\n        Gradientes dW1\u2026dWL, db1\u2026dbL normalizados por m.\n    \"\"\"\n    gradientes_parametros: dict[str, jnp.ndarray] = {}\n    numero_camadas = len(parametros_rede) // 2\n    m = rotulos_reais.shape[1]\n\n    A_saida = cache_propagacao[f\"A{numero_camadas}\"]\n    dZ = A_saida - rotulos_reais\n    gradientes_parametros[f\"dW{numero_camadas}\"] = (\n        dZ @ cache_propagacao[f\"A{numero_camadas - 1}\"].T\n    ) / m\n    gradientes_parametros[f\"db{numero_camadas}\"] = (\n        jnp.sum(dZ, axis=1, keepdims=True) / m\n    )\n    dA_prev = parametros_rede[f\"W{numero_camadas}\"].T @ dZ\n\n    mapa_derivadas = {\n        \"relu\": relu_derivada,\n        \"sigmoid\": sigmoid_derivada,\n        \"linear\": linear_derivada,\n    }\n    for camada in range(numero_camadas - 1, 0, -1):\n        Zc = cache_propagacao[f\"Z{camada}\"]\n        dZ = mapa_derivadas[nome_ativacao_oculta](dA_prev, Zc)\n        gradientes_parametros[f\"dW{camada}\"] = (\n            dZ @ cache_propagacao[f\"A{camada - 1}\"].T\n        ) / m\n        gradientes_parametros[f\"db{camada}\"] = jnp.sum(dZ, axis=1, keepdims=True) / m\n        if camada &gt; 1:\n            dA_prev = parametros_rede[f\"W{camada}\"].T @ dZ\n\n    return gradientes_parametros\n</code></pre>"},{"location":"reference/#ann.relu","title":"relu","text":"<pre><code>relu(entrada_linear)\n</code></pre> <p>Aplica a fun\u00e7\u00e3o de ativa\u00e7\u00e3o ReLU elemento a elemento.</p> <p>Parameters:</p> Name Type Description Default <code>entrada_linear</code> <code>ndarray</code> <p>Array de ativa\u00e7\u00f5es lineares (pr\u00e9-ativa\u00e7\u00e3o).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array com ReLU aplicada elemento a elemento.</p> Source code in <code>src\\ann.py</code> <pre><code>def relu(entrada_linear: jnp.ndarray) -&gt; jnp.ndarray:\n    \"\"\"\n    Aplica a fun\u00e7\u00e3o de ativa\u00e7\u00e3o ReLU elemento a elemento.\n\n    Parameters\n    ----------\n    entrada_linear : jnp.ndarray\n        Array de ativa\u00e7\u00f5es lineares (pr\u00e9-ativa\u00e7\u00e3o).\n\n    Returns\n    -------\n    jnp.ndarray\n        Array com ReLU aplicada elemento a elemento.\n    \"\"\"\n    return jnp.maximum(0, entrada_linear)\n</code></pre>"},{"location":"reference/#ann.relu_derivada","title":"relu_derivada","text":"<pre><code>relu_derivada(gradiente_saida, entrada_linear)\n</code></pre> <p>Calcula o gradiente da ReLU.</p> <p>Parameters:</p> Name Type Description Default <code>gradiente_saida</code> <code>ndarray</code> <p>Gradiente do custo em rela\u00e7\u00e3o \u00e0 sa\u00edda da ReLU.</p> required <code>entrada_linear</code> <code>ndarray</code> <p>Valor da entrada linear (antes da ReLU).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Gradiente do custo em rela\u00e7\u00e3o \u00e0 entrada linear.</p> Source code in <code>src\\ann.py</code> <pre><code>def relu_derivada(\n    gradiente_saida: jnp.ndarray, entrada_linear: jnp.ndarray\n) -&gt; jnp.ndarray:\n    \"\"\"\n    Calcula o gradiente da ReLU.\n\n    Parameters\n    ----------\n    gradiente_saida : jnp.ndarray\n        Gradiente do custo em rela\u00e7\u00e3o \u00e0 sa\u00edda da ReLU.\n    entrada_linear : jnp.ndarray\n        Valor da entrada linear (antes da ReLU).\n\n    Returns\n    -------\n    jnp.ndarray\n        Gradiente do custo em rela\u00e7\u00e3o \u00e0 entrada linear.\n    \"\"\"\n    return gradiente_saida * (entrada_linear &gt; 0)\n</code></pre>"},{"location":"reference/#ann.sigmoid","title":"sigmoid","text":"<pre><code>sigmoid(entrada_linear)\n</code></pre> <p>Aplica a fun\u00e7\u00e3o sigm\u00f3ide elemento a elemento.</p> <p>Parameters:</p> Name Type Description Default <code>entrada_linear</code> <code>ndarray</code> <p>Array de ativa\u00e7\u00f5es lineares (pr\u00e9-ativa\u00e7\u00e3o).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array com sigm\u00f3ide aplicada elemento a elemento.</p> Source code in <code>src\\ann.py</code> <pre><code>def sigmoid(entrada_linear: jnp.ndarray) -&gt; jnp.ndarray:\n    \"\"\"\n    Aplica a fun\u00e7\u00e3o sigm\u00f3ide elemento a elemento.\n\n    Parameters\n    ----------\n    entrada_linear : jnp.ndarray\n        Array de ativa\u00e7\u00f5es lineares (pr\u00e9-ativa\u00e7\u00e3o).\n\n    Returns\n    -------\n    jnp.ndarray\n        Array com sigm\u00f3ide aplicada elemento a elemento.\n    \"\"\"\n    return 1 / (1 + jnp.exp(-entrada_linear))\n</code></pre>"},{"location":"reference/#ann.sigmoid_derivada","title":"sigmoid_derivada","text":"<pre><code>sigmoid_derivada(gradiente_saida, entrada_linear)\n</code></pre> <p>Calcula o gradiente da fun\u00e7\u00e3o sigm\u00f3ide.</p> <p>Parameters:</p> Name Type Description Default <code>gradiente_saida</code> <code>ndarray</code> <p>Gradiente do custo em rela\u00e7\u00e3o \u00e0 sa\u00edda da sigm\u00f3ide.</p> required <code>entrada_linear</code> <code>ndarray</code> <p>Valor da entrada linear (antes da sigm\u00f3ide).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Gradiente do custo em rela\u00e7\u00e3o \u00e0 entrada linear.</p> Source code in <code>src\\ann.py</code> <pre><code>def sigmoid_derivada(\n    gradiente_saida: jnp.ndarray, entrada_linear: jnp.ndarray\n) -&gt; jnp.ndarray:\n    \"\"\"\n    Calcula o gradiente da fun\u00e7\u00e3o sigm\u00f3ide.\n\n    Parameters\n    ----------\n    gradiente_saida : jnp.ndarray\n        Gradiente do custo em rela\u00e7\u00e3o \u00e0 sa\u00edda da sigm\u00f3ide.\n    entrada_linear : jnp.ndarray\n        Valor da entrada linear (antes da sigm\u00f3ide).\n\n    Returns\n    -------\n    jnp.ndarray\n        Gradiente do custo em rela\u00e7\u00e3o \u00e0 entrada linear.\n    \"\"\"\n    saida_sigmoid = sigmoid(entrada_linear)\n    return gradiente_saida * saida_sigmoid * (1 - saida_sigmoid)\n</code></pre>"},{"location":"reference/#ann.softmax","title":"softmax","text":"<pre><code>softmax(entrada_linear)\n</code></pre> <p>Aplica softmax por coluna (cada amostra).</p> <p>Parameters:</p> Name Type Description Default <code>entrada_linear</code> <code>ndarray</code> <p>Array de ativa\u00e7\u00f5es lineares (dim: classes, n_amostras).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Distribui\u00e7\u00e3o de probabilidade para cada amostra (coluna).</p> Source code in <code>src\\ann.py</code> <pre><code>def softmax(entrada_linear: jnp.ndarray) -&gt; jnp.ndarray:\n    \"\"\"\n    Aplica softmax por coluna (cada amostra).\n\n    Parameters\n    ----------\n    entrada_linear : jnp.ndarray\n        Array de ativa\u00e7\u00f5es lineares (dim: classes, n_amostras).\n\n    Returns\n    -------\n    jnp.ndarray\n        Distribui\u00e7\u00e3o de probabilidade para cada amostra (coluna).\n    \"\"\"\n    exp_shift = jnp.exp(entrada_linear - jnp.max(entrada_linear, axis=0, keepdims=True))\n    return exp_shift / jnp.sum(exp_shift, axis=0, keepdims=True)\n</code></pre>"},{"location":"reference/#ann.softmax_derivada","title":"softmax_derivada","text":"<pre><code>softmax_derivada(\n    probabilidades_preditas, codificacao_one_hot\n)\n</code></pre> <p>Calcula o gradiente do erro categorial cruzado para softmax (sa\u00edda).</p> <p>Parameters:</p> Name Type Description Default <code>probabilidades_preditas</code> <code>ndarray</code> <p>Sa\u00eddas da softmax (probabilidades preditas).</p> required <code>codificacao_one_hot</code> <code>ndarray</code> <p>R\u00f3tulos em codifica\u00e7\u00e3o one-hot.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Gradiente da perda em rela\u00e7\u00e3o \u00e0 entrada linear da camada de sa\u00edda.</p> Source code in <code>src\\ann.py</code> <pre><code>def softmax_derivada(\n    probabilidades_preditas: jnp.ndarray, codificacao_one_hot: jnp.ndarray\n) -&gt; jnp.ndarray:\n    \"\"\"\n    Calcula o gradiente do erro categorial cruzado para softmax (sa\u00edda).\n\n    Parameters\n    ----------\n    probabilidades_preditas : jnp.ndarray\n        Sa\u00eddas da softmax (probabilidades preditas).\n    codificacao_one_hot : jnp.ndarray\n        R\u00f3tulos em codifica\u00e7\u00e3o one-hot.\n\n    Returns\n    -------\n    jnp.ndarray\n        Gradiente da perda em rela\u00e7\u00e3o \u00e0 entrada linear da camada de sa\u00edda.\n    \"\"\"\n    return probabilidades_preditas - codificacao_one_hot\n</code></pre>"},{"location":"reference/#ann.treinar_rede","title":"treinar_rede","text":"<pre><code>treinar_rede(\n    matriz_entrada,\n    matriz_rotulos,\n    dimensoes_camadas,\n    *,\n    nome_ativacao_oculta=\"relu\",\n    nome_ativacao_saida=\"sigmoid\",\n    nome_funcao_erro=\"erro_binario_cruzado\",\n    taxa_aprendizado=0.01,\n    numero_epocas=1000,\n    tamanho_lote=32,\n    semente=42,\n    verbose=True,\n)\n</code></pre> <p>Treina a rede neural artificial usando gradiente descendente e mini-batch.</p> <p>Parameters:</p> Name Type Description Default <code>matriz_entrada</code> <code>ndarray</code> <p>Dados de entrada, dimens\u00e3o (n_features, n_amostras).</p> required <code>matriz_rotulos</code> <code>ndarray</code> <p>R\u00f3tulos, dimens\u00e3o (n_classes, n_amostras) ou (1, n_amostras) para bin\u00e1rio/regress\u00e3o.</p> required <code>dimensoes_camadas</code> <code>Sequence[int]</code> <p>Arquitetura da rede incluindo entrada e sa\u00edda.</p> required <code>nome_ativacao_oculta</code> <code>('relu', 'sigmoid', 'tanh')</code> <p>Fun\u00e7\u00e3o de ativa\u00e7\u00e3o das camadas ocultas.</p> <code>'relu'</code> <code>nome_ativacao_saida</code> <code>('sigmoid', 'softmax', 'linear')</code> <p>Fun\u00e7\u00e3o de ativa\u00e7\u00e3o da sa\u00edda.</p> <code>'sigmoid'</code> <code>nome_funcao_erro</code> <code>('erro_binario_cruzado', 'erro_categorial_cruzado', 'erro_mse')</code> <p>Nome da fun\u00e7\u00e3o de erro.</p> <code>'erro_binario_cruzado'</code> <code>taxa_aprendizado</code> <code>(float, opcional)</code> <p>Taxa de atualiza\u00e7\u00e3o do gradiente.</p> <code>0.01</code> <code>numero_epocas</code> <code>(int, opcional)</code> <p>N\u00famero de \u00e9pocas de treinamento.</p> <code>1000</code> <code>tamanho_lote</code> <code>(int, opcional)</code> <p>Tamanho do mini-batch.</p> <code>32</code> <code>semente</code> <code>(int, opcional)</code> <p>Semente para reprodutibilidade.</p> <code>42</code> <code>verbose</code> <code>(bool, opcional)</code> <p>Se True, imprime progresso.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>parametros_rede</code> <code>dict[str, ndarray]</code> <p>Parameters aprendidos ao final do treinamento.</p> Source code in <code>src\\ann.py</code> <pre><code>def treinar_rede(\n    matriz_entrada: jnp.ndarray,\n    matriz_rotulos: jnp.ndarray,\n    dimensoes_camadas: Sequence[int],\n    *,\n    nome_ativacao_oculta: str = \"relu\",\n    nome_ativacao_saida: str = \"sigmoid\",\n    nome_funcao_erro: str = \"erro_binario_cruzado\",\n    taxa_aprendizado: float = 0.01,\n    numero_epocas: int = 1000,\n    tamanho_lote: int = 32,\n    semente: int = 42,\n    verbose: bool = True,\n) -&gt; dict[str, jnp.ndarray]:\n    \"\"\"\n    Treina a rede neural artificial usando gradiente descendente e mini-batch.\n\n    Parameters\n    ----------\n    matriz_entrada : jnp.ndarray\n        Dados de entrada, dimens\u00e3o (n_features, n_amostras).\n    matriz_rotulos : jnp.ndarray\n        R\u00f3tulos, dimens\u00e3o (n_classes, n_amostras) ou (1, n_amostras) para bin\u00e1rio/regress\u00e3o.\n    dimensoes_camadas : Sequence[int]\n        Arquitetura da rede incluindo entrada e sa\u00edda.\n    nome_ativacao_oculta : {'relu', 'sigmoid', 'tanh'}, opcional\n        Fun\u00e7\u00e3o de ativa\u00e7\u00e3o das camadas ocultas.\n    nome_ativacao_saida : {'sigmoid', 'softmax', 'linear'}, opcional\n        Fun\u00e7\u00e3o de ativa\u00e7\u00e3o da sa\u00edda.\n    nome_funcao_erro : {'erro_binario_cruzado', 'erro_categorial_cruzado', 'erro_mse'}, opcional\n        Nome da fun\u00e7\u00e3o de erro.\n    taxa_aprendizado : float, opcional\n        Taxa de atualiza\u00e7\u00e3o do gradiente.\n    numero_epocas : int, opcional\n        N\u00famero de \u00e9pocas de treinamento.\n    tamanho_lote : int, opcional\n        Tamanho do mini-batch.\n    semente : int, opcional\n        Semente para reprodutibilidade.\n    verbose : bool, opcional\n        Se True, imprime progresso.\n\n    Returns\n    -------\n    parametros_rede : dict[str, jnp.ndarray]\n        Parameters aprendidos ao final do treinamento.\n    \"\"\"\n    chave_aleatoria = definir_semente(semente)\n    parametros_rede, chave_aleatoria = inicializar_parametros_rede(\n        dimensoes_camadas,\n        chave_aleatoria=chave_aleatoria,\n        nome_ativacao_oculta=nome_ativacao_oculta,\n    )\n\n    numero_amostras = matriz_entrada.shape[1]\n    historico_erros: list[float] = []\n\n    funcoes_erro: dict[str, Callable[[jnp.ndarray, jnp.ndarray], float]] = {\n        \"erro_binario_cruzado\": erro_binario_cruzado,\n        \"erro_categorial_cruzado\": erro_categorial_cruzado,\n        \"erro_mse\": erro_mse,\n    }\n    calcular_erro = funcoes_erro[nome_funcao_erro]\n\n    for epoca in range(1, numero_epocas + 1):\n        chave_aleatoria, chave_sub = jax.random.split(chave_aleatoria)\n        indices_embaralhados = jax.random.permutation(chave_sub, numero_amostras)\n        for inicio in range(0, numero_amostras, tamanho_lote):\n            indices_lote = indices_embaralhados[inicio : inicio + tamanho_lote]\n            entradas_lote = matriz_entrada[:, indices_lote]\n            rotulos_lote = matriz_rotulos[:, indices_lote]\n\n            predicoes_lote, cache_propagacao = propagacao_ante(\n                entradas_lote,\n                parametros_rede,\n                nome_ativacao_oculta=nome_ativacao_oculta,\n                nome_ativacao_saida=nome_ativacao_saida,\n            )\n            erro_lote = calcular_erro(predicoes_lote, rotulos_lote)\n            gradientes_parametros = propagacao_retro(\n                parametros_rede,\n                cache_propagacao,\n                rotulos_lote,\n                nome_ativacao_oculta=nome_ativacao_oculta,\n            )\n            parametros_rede = atualizar_parametros(\n                parametros_rede, gradientes_parametros, taxa_aprendizado\n            )\n        historico_erros.append(erro_lote)\n        if verbose and epoca % max(1, numero_epocas // 10) == 0:\n            print(f\"\u00c9poca {epoca:&gt;4}/{numero_epocas} \u2013 erro: {erro_lote:.6f}\")\n    return parametros_rede\n</code></pre>"},{"location":"examples/bank/","title":"Binary Classification of Term Deposit Subscription","text":"<p>This notebook implements a binary classification pipeline using a neural network built with JAX. The model predicts whether a customer will subscribe to a term deposit, based on features from the Bank Marketing dataset.</p> <p>Dataset source: Bank Marketing Dataset on Kaggle</p> <pre><code>import jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\nimport ann\n</code></pre>"},{"location":"examples/bank/#load-and-prepare-data","title":"Load and prepare data","text":"<p>Target variable is <code>deposit</code>, which is mapped to 1 for 'yes' and 0 for 'no'.</p> <pre><code>df = pd.read_csv(\"datasets/bank.csv\")\ndf[\"y\"] = df[\"deposit\"].map({\"yes\": 1, \"no\": 0})\ndf = df.drop(columns=[\"deposit\"])\n</code></pre>"},{"location":"examples/bank/#preprocessing","title":"Preprocessing","text":"<ul> <li>One-hot encoding for categorical columns.</li> <li>Min-max normalization for selected numeric columns.</li> <li>Concatenation of both feature types into the input matrix.</li> </ul> <pre><code>CAT_COLS = [\n    \"job\",\n    \"marital\",\n    \"education\",\n    \"default\",\n    \"housing\",\n    \"loan\",\n    \"contact\",\n    \"month\",\n    \"day\",\n    \"poutcome\",\n]\nNUM_COLS = [\"age\", \"balance\", \"campaign\"]\n\ndf_cat = pd.get_dummies(df[CAT_COLS], drop_first=True).astype(float)\n\nX_num = df[NUM_COLS].astype(float).values\nX_min = X_num.min(axis=0, keepdims=True)\nX_max = X_num.max(axis=0, keepdims=True)\nX_num_norm = (X_num - X_min) / (X_max - X_min + 1e-8)\ndf_num = pd.DataFrame(X_num_norm, columns=NUM_COLS, index=df.index)\n\nX = pd.concat([df_num, df_cat], axis=1).astype(float).values\ny = df[\"y\"].values.reshape(1, -1)\n</code></pre>"},{"location":"examples/bank/#train-test-split","title":"Train-test split","text":"<p>85% of the data is used for training and 15% for testing, with stratification based on the target label.</p> <pre><code>X_train, X_test, y_train, y_test = train_test_split(\n    X, y.T, test_size=0.15, random_state=42, stratify=y.T\n)\n\nX_train = jnp.array(X_train.T)\nX_test = jnp.array(X_test.T)\ny_train = jnp.array(y_train.T)\ny_test = jnp.array(y_test.T)\n</code></pre>"},{"location":"examples/bank/#neural-network-training","title":"Neural network training","text":"<p>Architecture: Input \u2192 64 \u2192 16 \u2192 1. Output layer uses sigmoid activation for binary classification.</p> <pre><code>params = ann.treinar_rede(\n    matriz_entrada=X_train,\n    matriz_rotulos=y_train,\n    dimensoes_camadas=(X_train.shape[0], 64, 16, 1),\n    nome_ativacao_oculta=\"relu\",\n    nome_ativacao_saida=\"sigmoid\",\n    nome_funcao_erro=\"erro_binario_cruzado\",\n    taxa_aprendizado=0.01,\n    numero_epocas=1000,\n    tamanho_lote=128,\n    dropout_prob=0.0,\n    semente=42,\n    verbose=True,\n)\n</code></pre>"},{"location":"examples/bank/#performance-evaluation","title":"Performance evaluation","text":"<p>Includes accuracy metrics and confusion matrix for both training and test data.</p> <pre><code>y_pred_train = ann.prever(\n    X_train, params, nome_ativacao_oculta=\"relu\", nome_ativacao_saida=\"sigmoid\"\n)\ny_pred_test = ann.prever(\n    X_test, params, nome_ativacao_oculta=\"relu\", nome_ativacao_saida=\"sigmoid\"\n)\n\nacc_train = accuracy_score(np.array(y_train).flatten(), np.array(y_pred_train).flatten())\nacc_test = accuracy_score(np.array(y_test).flatten(), np.array(y_pred_test).flatten())\n\ntn, fp, fn, tp = confusion_matrix(np.array(y_test).flatten(), np.array(y_pred_test).flatten()).ravel()\n\nprint(f\"Training accuracy: {acc_train:.4f}\")\nprint(f\"Test accuracy:     {acc_test:.4f}\")\nprint(f\"False Positives: {fp}, False Negatives: {fn}\")\n</code></pre>"},{"location":"examples/bank/#confusion-matrix-visualization","title":"Confusion matrix visualization","text":"<pre><code>cm = np.array([[tn, fp], [fn, tp]])\nplt.figure(figsize=(4, 4))\nplt.imshow(cm, cmap=\"Blues\")\nfor i in range(2):\n    for j in range(2):\n        plt.text(\n            j, i, cm[i, j],\n            ha=\"center\", va=\"center\",\n            color=\"white\" if cm[i, j] &gt; cm.max() / 2 else \"black\",\n            fontsize=12\n        )\nplt.xticks([0, 1], [\"Negative\", \"Positive\"])\nplt.yticks([0, 1], [\"Negative\", \"Positive\"])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix - Test Set\")\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"examples/real_state/","title":"Regress\u00e3o Imobili\u00e1ria","text":"<p>Este notebook implementa um pipeline de regress\u00e3o usando uma rede neural constru\u00edda com JAX. O modelo prev\u00ea pre\u00e7os de im\u00f3veis com base em caracter\u00edsticas do conjunto de dados de mercado imobili\u00e1rio.</p> <p>Dataset source: Housing Prices Regression Dataset on Kaggle</p>"},{"location":"examples/real_state/#importacao-de-bibliotecas","title":"Importa\u00e7\u00e3o de bibliotecas","text":"<ul> <li><code>jax.numpy</code> para opera\u00e7\u00f5es vetorizadas com JAX.</li> <li><code>matplotlib.pyplot</code> para visualiza\u00e7\u00f5es.</li> <li><code>numpy</code> e <code>pandas</code> para manipula\u00e7\u00e3o de dados.</li> <li><code>r2_score</code> do scikit-learn para avaliar desempenho.</li> <li><code>train_test_split</code> para particionar dados.</li> <li><code>src.ann</code> para fun\u00e7\u00f5es de rede neural.</li> </ul> <pre><code>import jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\n\nimport ann\n</code></pre>"},{"location":"examples/real_state/#carregamento-e-preparacao-dos-dados","title":"Carregamento e prepara\u00e7\u00e3o dos dados","text":"<ul> <li>L\u00ea o CSV do conjunto de dados.</li> <li>Separa as features (<code>X_df</code>) removendo <code>ID</code> e <code>Price</code>.</li> <li>Extrai o vetor de pre\u00e7os (<code>y</code>) e coloca em formato de matriz.</li> </ul> <pre><code>df = pd.read_csv(\"datasets/real_estate_dataset.csv\")\n\nX_df = df.drop(columns=[\"ID\", \"Price\"])\ny = df[\"Price\"].values.reshape(1, -1)\nX = X_df.values.T\n</code></pre>"},{"location":"examples/real_state/#pre-processamento","title":"Pr\u00e9-processamento","text":"<ul> <li>Normaliza\u00e7\u00e3o min-max de cada feature de entrada.</li> <li>Normaliza\u00e7\u00e3o do alvo (pre\u00e7o) para faixa [0,1].</li> </ul> <pre><code>X_min = X.min(axis=1, keepdims=True)\nX_max = X.max(axis=1, keepdims=True)\nX_norm = (X - X_min) / (X_max - X_min + 1e-8)\ny_min = y.min()\ny_max = y.max()\ny_norm = (y - y_min) / (y_max - y_min + 1e-8)\n</code></pre>"},{"location":"examples/real_state/#divisao-treino-teste","title":"Divis\u00e3o treino-teste","text":"<ul> <li>80% dos dados para treino e 20% para teste.</li> <li>Usa <code>train_test_split</code> com <code>random_state=1</code> para reprodutibilidade.</li> </ul> <pre><code>X_train, X_test, y_train, y_test = train_test_split(\n    X_norm.T, y_norm.T, test_size=0.2, random_state=1\n)\nX_train = jnp.array(X_train.T)\nX_test = jnp.array(X_test.T)\ny_train = jnp.array(y_train.T)\ny_test = jnp.array(y_test.T)\n\nprint(\"Shapes de treino:\", X_train.shape, y_train.shape)\n</code></pre>"},{"location":"examples/real_state/#treinamento-da-rede-neural","title":"Treinamento da rede neural","text":"<ul> <li>Arquitetura: Entrada \u2192 10 neur\u00f4nios ocultos \u2192 1 sa\u00edda.</li> <li>Ativa\u00e7\u00e3o oculta: ReLU; sa\u00edda: Linear.</li> <li>Fun\u00e7\u00e3o de erro: MSE.</li> <li>Taxa de aprendizado, n\u00famero de \u00e9pocas e tamanho do lote definidos.</li> </ul> <pre><code>params = ann.treinar_rede(\n    matriz_entrada=X_train,\n    matriz_rotulos=y_train,\n    dimensoes_camadas=(\n        X_train.shape[0],\n        10,\n        1,\n    ),\n    nome_ativacao_oculta=\"relu\",\n    nome_ativacao_saida=\"linear\",\n    nome_funcao_erro=\"erro_mse\",\n    taxa_aprendizado=0.05,\n    numero_epocas=2000,\n    tamanho_lote=16,\n    verbose=True,\n)\n</code></pre>"},{"location":"examples/real_state/#predicao","title":"Predi\u00e7\u00e3o","text":"<ul> <li>Gera previs\u00f5es normalizadas para treino e teste.</li> </ul> <pre><code>y_pred_train = ann.prever(\n    X_train, params, nome_ativacao_oculta=\"relu\", nome_ativacao_saida=\"linear\"\n)\ny_pred_test = ann.prever(\n    X_test, params, nome_ativacao_oculta=\"relu\", nome_ativacao_saida=\"linear\"\n)\n</code></pre>"},{"location":"examples/real_state/#denormalizacao-dos-resultados","title":"Denormaliza\u00e7\u00e3o dos resultados","text":"<ul> <li>Converte as previs\u00f5es normalizadas de volta para valores reais de pre\u00e7o.</li> </ul> <pre><code>y_pred_train_real = np.array(y_pred_train) * (y_max - y_min) + y_min\ny_train_real = np.array(y_train) * (y_max - y_min) + y_min\ny_pred_test_real = np.array(y_pred_test) * (y_max - y_min) + y_min\ny_test_real = np.array(y_test) * (y_max - y_min) + y_min\n</code></pre>"},{"location":"examples/real_state/#avaliacao-de-desempenho","title":"Avalia\u00e7\u00e3o de desempenho","text":"<ul> <li>Calcula MSE e R\u00b2 para teste usando <code>r2_score</code>.</li> </ul> <pre><code>mse_test = np.mean((y_pred_test_real - y_test_real) ** 2)\nr2_test = r2_score(y_test_real.flatten(), y_pred_test_real.flatten())\n\nprint(f\"MSE teste  (real): {mse_test:.2f}\")\nprint(f\"R\u00b2 teste:          {r2_test:.4f}\")\n</code></pre>"},{"location":"examples/real_state/#visualizacao-real-vs-predito","title":"Visualiza\u00e7\u00e3o: Real vs. Predito","text":"<ul> <li>Gr\u00e1fico de dispers\u00e3o comparando pre\u00e7os reais e previstos no conjunto de teste.</li> </ul> <pre><code>plt.figure(figsize=(7, 6))\nplt.scatter(y_test_real.flatten(), y_pred_test_real.flatten(), alpha=0.7, edgecolor=\"k\")\nplt.plot(\n    [y_test_real.min(), y_test_real.max()],\n    [y_test_real.min(), y_test_real.max()],\n    \"r--\",\n    lw=2,\n)\nplt.xlabel(\"Pre\u00e7o real\")\nplt.ylabel(\"Pre\u00e7o predito pela RNA\")\nplt.title(\"Regress\u00e3o Imobili\u00e1ria: Real vs. Predito (teste)\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n</code></pre>"}]}